{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c5c9b",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Here, I want to try to re-implement the whole WordDetectorNN in a single Jupyter Notebook to keep things simple. Let's see if I get that done :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from my_code import IAM_Dataset\n",
    "from my_code import ImageDimensions\n",
    "from my_code import custom_collate_fn\n",
    "from my_code import count_parameters\n",
    "from my_code import fg_by_cc\n",
    "from my_code import cluster_aabbs\n",
    "from my_code import binary_classification_metrics\n",
    "from my_code import draw_bboxes_on_image\n",
    "from my_code import MapOrdering\n",
    "from my_code import encode, decode, BoundingBox, ImageDimensions\n",
    "from my_code import ModifiedResNet18\n",
    "from my_code import WordDetectorNet\n",
    "from my_code import compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559f3ce",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Here, I note down how I build the project to remind myself and others in the future. Here we go:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IAM Folder] --> B[Train Dataset = Dtr]\n",
    "    A --> C[Val Dataset = Dval]\n",
    "    B --> D[Dtr w transform, img&aabb = Dtr']\n",
    "    C --> E[Dval w transform, img&aabb = Dval']\n",
    "    D --> F[Train DataLoader] \n",
    "    E --> G[Val DataLoader] \n",
    "    E --> H[no transform except normalisation]\n",
    "    D --> I[geometric & photo]\n",
    "```\n",
    "\n",
    "- transform in Dataset\n",
    "- encode in collate of DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad157a94",
   "metadata": {},
   "source": [
    "## Encoding & Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc74a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aabbs = [\n",
    "    # BoundingBox( 0,  0, 10, 10, label='word1'),\n",
    "    # BoundingBox( 5,  5, 15, 15, label='word2'),\n",
    "    BoundingBox(20, 20, 30, 30, label='word3'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = ImageDimensions(100, 100)\n",
    "output_size = ImageDimensions(50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec055923",
   "metadata": {},
   "outputs": [],
   "source": [
    "aabbs_encoded = encode(input_size, output_size, aabbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a214d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(aabbs_encoded[MapOrdering.SEG_WORD, :, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert input_size.width / output_size.width == input_size.height / output_size.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_aabbs = decode(aabbs_encoded, scale=input_size.width / output_size.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_aabbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2961b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc41c4",
   "metadata": {},
   "source": [
    "First, create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment w/ dataset class\n",
    "data_path = Path.home() / 'Development/WordDetectorNN/data/train'\n",
    "dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=True,\n",
    "    transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a61a88",
   "metadata": {},
   "source": [
    "Next, access an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 578\n",
    "idx = 0\n",
    "idx = 325\n",
    "sample = dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8f404",
   "metadata": {},
   "source": [
    "Then, plot a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.store_element_as_image(idx, Path('test.png'), draw_bboxes=True, store_gt_encoded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1bab0c",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into training and val datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way to create the train and val datasets seems convoluted but is necessary to ensure\n",
    "# that train and val get only their transforms. I know that it could be implemented more efficiently\n",
    "# but that's not necessary give the small dataset.\n",
    "#\n",
    "# An alternative way to implement it is to build a TransformSubset which not only Subset's but also\n",
    "# applies a separate transform.\n",
    "#\n",
    "# Note that it is not a good idea to hardcode these transforms b/c one might want to use the plain dataset,\n",
    "# even if only for inspection\n",
    "\n",
    "# Create datasets with different transforms\n",
    "train_transform = None\n",
    "val_transform = None\n",
    "# TODO: ^ Implement the augmentations, w/ each changing at every batch\n",
    "\n",
    "train_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=False,\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "percent_train_data = 80\n",
    "\n",
    "assert len(train_dataset) == len(val_dataset)\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split = int(percent_train_data / 100 * len(indices))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_filenames = [sample['filename'] for sample in train_subset]\n",
    "val_filenames = [sample['filename'] for sample in val_subset]\n",
    "# Check that no train samples are in val\n",
    "assert len(set(train_filenames + val_filenames)) == len(train_filenames) + len(val_filenames)\n",
    "\n",
    "assert len(dataset) == len(train_subset) + len(val_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57d696",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data_loader = True\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27404d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176d44a",
   "metadata": {},
   "source": [
    "Check lenghts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_train), len(train_subset), len(train_subset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360025b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_val), len(val_subset), len(val_subset) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a53d3",
   "metadata": {},
   "source": [
    "Load a single batch for testing & inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = next(iter(dataloader_train))\n",
    "batch_val = next(iter(dataloader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fff7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train.keys(), batch_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe460c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train['images'].shape, batch_val['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train['gt_encoded'].shape, batch_val['gt_encoded'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af839ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_train['bounding_boxes']), len(batch_val['bounding_boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1becb",
   "metadata": {},
   "source": [
    "Iterate through whole dataloader once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader_train:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d444e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader_val:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0120",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5009",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ModifiedResNet18()\n",
    "\n",
    "H, W = 400, 500\n",
    "H, W = 448, 448\n",
    "H, W = 600, 600\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = backbone(test_input)\n",
    "out5, out4, out3, out2, out1 = output\n",
    "\n",
    "print(\"Print output sizes:\")\n",
    "for o in output:\n",
    "    print(\"\\t\", o.shape)\n",
    "\n",
    "nr_params = count_parameters(backbone)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ef8e",
   "metadata": {},
   "source": [
    "Now off to the `WordDetectorNN` (for now just copied from external repo):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f4992",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "H, W = net.input_size\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = net(test_input)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "\n",
    "nr_params = count_parameters(net)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96fd07",
   "metadata": {},
   "source": [
    "Test neural network with dataloader item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_item = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8263222",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    input_size=ImageDimensions(width=448, height=448),\n",
    "    output_size=ImageDimensions(width=224, height=224),\n",
    "    force_rebuild_cache=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "shuffle_data_loader = True\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_item = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ade82",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "output = net(batch_item['images'])\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "print(\"`gt_encoded` shape:\", batch_item['gt_encoded'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2a7bb",
   "metadata": {},
   "source": [
    "It turns out that we need that `1` dimension in the input because the backbone convolutional uses this.\n",
    "\n",
    "Next, confirm that the above forward pass also works on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "net.to('cuda')\n",
    "\n",
    "images = batch_item['images']\n",
    "images = images.to('cuda')\n",
    "\n",
    "output = net(images)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "print(\"`gt_encoded` shape:\", batch_item['gt_encoded'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c6f7a",
   "metadata": {},
   "source": [
    "Yes, this seems to work, great! Interestingly, it is much faster than on CPU: 14.0s vs 0.1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c1805",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a0631",
   "metadata": {},
   "source": [
    "For now, just copied to first make it work and improve the implementation (maybe) later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6699622",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = output.to('cuda')\n",
    "gt_map = batch_item['gt_encoded'].to('cuda')\n",
    "l = compute_loss(y, gt_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42806e9",
   "metadata": {},
   "source": [
    "OK, this seems to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
