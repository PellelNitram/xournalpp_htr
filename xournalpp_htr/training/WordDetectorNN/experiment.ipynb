{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c5c9b",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Here, I want to try to re-implement the whole WordDetectorNN in a single Jupyter Notebook to keep things simple. Let's see if I get that done :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1d469",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34994a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([3], device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c54b2",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(net):\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0120",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you were using Bottleneck for other ResNet versions:\n",
    "# from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "class ModifiedResNet18(ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Initialize with BasicBlock and standard ResNet-18 layers\n",
    "        # num_classes is irrelevant here as we won't use the fc layer\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2], num_classes=1000, **kwargs)\n",
    "\n",
    "        # 1. Modify the first convolutional layer for 1-channel (grayscale) input\n",
    "        # Original resnet.conv1 is Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # We need Conv2d(1, 64, ...)\n",
    "        original_conv1 = self.conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1,\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False,\n",
    "        )  # bias is False in original ResNet conv1\n",
    "\n",
    "        # Optional: If you wanted to initialize weights similarly to torchvision:\n",
    "        # nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # However, if you load custom pretrained weights for the whole model later,\n",
    "        # this specific initialization might be overwritten.\n",
    "\n",
    "        # We don't need the final fully connected layer for feature extraction\n",
    "        del self.fc\n",
    "        # self.avgpool is also not strictly needed for the U-Net style features,\n",
    "        # but it doesn't hurt to keep it if not used. You could 'del self.avgpool' too.\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor):\n",
    "        # This is largely copied from torchvision.models.resnet.ResNet._forward_impl\n",
    "        # but modified to return intermediate features.\n",
    "\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        out1 = self.relu(x)  # Corresponds to bb1 in WordDetectorNet (before maxpool)\n",
    "        x = self.maxpool(out1)\n",
    "\n",
    "        out2 = self.layer1(x)  # Corresponds to bb2\n",
    "        out3 = self.layer2(out2)  # Corresponds to bb3\n",
    "        out4 = self.layer3(out3)  # Corresponds to bb4\n",
    "        out5 = self.layer4(out4)  # Corresponds to bb5\n",
    "\n",
    "        # WordDetectorNet expects (bb5, bb4, bb3, bb2, bb1)\n",
    "        return out5, out4, out3, out2, out1\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5009",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ModifiedResNet18()\n",
    "\n",
    "H, W = 400, 500\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = backbone(test_input)\n",
    "out5, out4, out3, out2, out1 = output\n",
    "\n",
    "print(\"Print output sizes:\")\n",
    "for o in output:\n",
    "    print(\"\\t\", o.shape)\n",
    "\n",
    "nr_params = count_parameters(backbone)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ef8e",
   "metadata": {},
   "source": [
    "Now off to the `WordDetectorNN` (for now just copied from external repo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapOrdering:\n",
    "    \"\"\"order of the maps encoding the aabbs around the words\"\"\"\n",
    "\n",
    "    SEG_WORD = 0\n",
    "    SEG_SURROUNDING = 1\n",
    "    SEG_BACKGROUND = 2\n",
    "    GEO_TOP = 3\n",
    "    GEO_BOTTOM = 4\n",
    "    GEO_LEFT = 5\n",
    "    GEO_RIGHT = 6\n",
    "    NUM_MAPS = 7\n",
    "\n",
    "\n",
    "def compute_scale_down(input_size, output_size):\n",
    "    \"\"\"compute scale down factor of neural network, given input and output size\"\"\"\n",
    "    return output_size[0] / input_size[0]\n",
    "\n",
    "\n",
    "class UpscaleAndConcatLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    take small map with cx channels\n",
    "    upscale to size of large map (s*s)\n",
    "    concat large map with cy channels and upscaled small map\n",
    "    apply conv and output map with cz channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cx, cy, cz):\n",
    "        super(UpscaleAndConcatLayer, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(cx + cy, cz, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, y, s):\n",
    "        x = F.interpolate(x, s)\n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = F.relu(self.conv(z))\n",
    "        return z\n",
    "\n",
    "\n",
    "class WordDetectorNet(torch.nn.Module):\n",
    "    input_size = (448, 448)\n",
    "    output_size = (224, 224)\n",
    "    scale_down = compute_scale_down(input_size, output_size)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WordDetectorNet, self).__init__()\n",
    "\n",
    "        # Use the modified ResNet18 for feature extraction\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        # All weights in the backbone will be randomly initialized.\n",
    "\n",
    "        self.up1 = UpscaleAndConcatLayer(512, 256, 256)  # input//16\n",
    "        self.up2 = UpscaleAndConcatLayer(256, 128, 128)  # input//8\n",
    "        self.up3 = UpscaleAndConcatLayer(128, 64, 64)  # input//4\n",
    "        self.up4 = UpscaleAndConcatLayer(64, 64, 32)  # input//2\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(32, MapOrdering.NUM_MAPS, 3, 1, padding=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_shape(s, f):\n",
    "        assert s[0] % f == 0 and s[1] % f == 0\n",
    "        return s[0] // f, s[1] // f\n",
    "\n",
    "    def output_activation(self, x, apply_softmax):\n",
    "        if apply_softmax:\n",
    "            seg = torch.softmax(\n",
    "                x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            seg = x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1]\n",
    "        geo = torch.sigmoid(x[:, MapOrdering.GEO_TOP :]) * self.input_size[0]\n",
    "        y = torch.cat([seg, geo], dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        s = x.shape[2:]  # Original image shape HxW\n",
    "        bb5, bb4, bb3, bb2, bb1 = self.backbone(x)\n",
    "\n",
    "        y = self.up1(bb5, bb4, self.scale_shape(s, 16))\n",
    "        # up2 takes y (H/16, 256ch) and bb3 (H/8, 128ch). Upscales y to H/8. Output: H/8, 128ch.\n",
    "        y = self.up2(y, bb3, self.scale_shape(s, 8))\n",
    "        # up3 takes y (H/8, 128ch) and bb2 (H/4, 64ch). Upscales y to H/4. Output: H/4, 64ch.\n",
    "        y = self.up3(y, bb2, self.scale_shape(s, 4))\n",
    "        # up4 takes y (H/4, 64ch) and bb1 (H/2, 64ch). Upscales y to H/2. Output: H/2, 32ch.\n",
    "        y = self.up4(y, bb1, self.scale_shape(s, 2))\n",
    "\n",
    "        y = self.conv1(\n",
    "            y\n",
    "        )  # Final convolution to get NUM_MAPS channels. Output: H/2, NUM_MAPS ch.\n",
    "\n",
    "        return self.output_activation(y, apply_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f4992",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "H, W = net.input_size\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = net(test_input)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "\n",
    "nr_params = count_parameters(net)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e83a97",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4efebb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xournalpp_htr_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
