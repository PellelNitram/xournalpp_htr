{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c5c9b",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Here, I want to try to re-implement the whole WordDetectorNN in a single Jupyter Notebook to keep things simple. Let's see if I get that done :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1d469",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34994a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([3], device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559f3ce",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Here, I note down how I build the project to remind myself and others in the future. Here we go:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IAM Folder] --> B[Train Dataset = Dtr]\n",
    "    A --> C[Val Dataset = Dval]\n",
    "    B --> D[Dtr w transform, img&aabb = Dtr']\n",
    "    C --> E[Dval w transform, img&aabb = Dval']\n",
    "    D --> F[Train DataLoader] \n",
    "    E --> G[Val DataLoader] \n",
    "    E --> H[no transform except normalisation]\n",
    "    D --> I[geometric & photo]\n",
    "```\n",
    "\n",
    "- transform in Dataset\n",
    "- encode in collate of DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c54b2",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(net):\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0120",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you were using Bottleneck for other ResNet versions:\n",
    "# from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "class ModifiedResNet18(ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Initialize with BasicBlock and standard ResNet-18 layers\n",
    "        # num_classes is irrelevant here as we won't use the fc layer\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2], num_classes=1000, **kwargs)\n",
    "\n",
    "        # 1. Modify the first convolutional layer for 1-channel (grayscale) input\n",
    "        # Original resnet.conv1 is Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # We need Conv2d(1, 64, ...)\n",
    "        original_conv1 = self.conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1,\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False,\n",
    "        )  # bias is False in original ResNet conv1\n",
    "\n",
    "        # Optional: If you wanted to initialize weights similarly to torchvision:\n",
    "        # nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # However, if you load custom pretrained weights for the whole model later,\n",
    "        # this specific initialization might be overwritten.\n",
    "\n",
    "        # We don't need the final fully connected layer for feature extraction\n",
    "        del self.fc\n",
    "        # self.avgpool is also not strictly needed for the U-Net style features,\n",
    "        # but it doesn't hurt to keep it if not used. You could 'del self.avgpool' too.\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor):\n",
    "        # This is largely copied from torchvision.models.resnet.ResNet._forward_impl\n",
    "        # but modified to return intermediate features.\n",
    "\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        out1 = self.relu(x)  # Corresponds to bb1 in WordDetectorNet (before maxpool)\n",
    "        x = self.maxpool(out1)\n",
    "\n",
    "        out2 = self.layer1(x)  # Corresponds to bb2\n",
    "        out3 = self.layer2(out2)  # Corresponds to bb3\n",
    "        out4 = self.layer3(out3)  # Corresponds to bb4\n",
    "        out5 = self.layer4(out4)  # Corresponds to bb5\n",
    "\n",
    "        # WordDetectorNet expects (bb5, bb4, bb3, bb2, bb1)\n",
    "        return out5, out4, out3, out2, out1\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5009",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ModifiedResNet18()\n",
    "\n",
    "H, W = 400, 500\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = backbone(test_input)\n",
    "out5, out4, out3, out2, out1 = output\n",
    "\n",
    "print(\"Print output sizes:\")\n",
    "for o in output:\n",
    "    print(\"\\t\", o.shape)\n",
    "\n",
    "nr_params = count_parameters(backbone)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ef8e",
   "metadata": {},
   "source": [
    "Now off to the `WordDetectorNN` (for now just copied from external repo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapOrdering:\n",
    "    \"\"\"order of the maps encoding the aabbs around the words\"\"\"\n",
    "\n",
    "    SEG_WORD = 0\n",
    "    SEG_SURROUNDING = 1\n",
    "    SEG_BACKGROUND = 2\n",
    "    GEO_TOP = 3\n",
    "    GEO_BOTTOM = 4\n",
    "    GEO_LEFT = 5\n",
    "    GEO_RIGHT = 6\n",
    "    NUM_MAPS = 7\n",
    "\n",
    "\n",
    "def compute_scale_down(input_size, output_size):\n",
    "    \"\"\"compute scale down factor of neural network, given input and output size\"\"\"\n",
    "    return output_size[0] / input_size[0]\n",
    "\n",
    "\n",
    "class UpscaleAndConcatLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    take small map with cx channels\n",
    "    upscale to size of large map (s*s)\n",
    "    concat large map with cy channels and upscaled small map\n",
    "    apply conv and output map with cz channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cx, cy, cz):\n",
    "        super(UpscaleAndConcatLayer, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(cx + cy, cz, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, y, s):\n",
    "        x = F.interpolate(x, s)\n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = F.relu(self.conv(z))\n",
    "        return z\n",
    "\n",
    "\n",
    "class WordDetectorNet(torch.nn.Module):\n",
    "    input_size = (448, 448)\n",
    "    output_size = (224, 224)\n",
    "    scale_down = compute_scale_down(input_size, output_size)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WordDetectorNet, self).__init__()\n",
    "\n",
    "        # Use the modified ResNet18 for feature extraction\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        # All weights in the backbone will be randomly initialized.\n",
    "\n",
    "        self.up1 = UpscaleAndConcatLayer(512, 256, 256)  # input//16\n",
    "        self.up2 = UpscaleAndConcatLayer(256, 128, 128)  # input//8\n",
    "        self.up3 = UpscaleAndConcatLayer(128, 64, 64)  # input//4\n",
    "        self.up4 = UpscaleAndConcatLayer(64, 64, 32)  # input//2\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(32, MapOrdering.NUM_MAPS, 3, 1, padding=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_shape(s, f):\n",
    "        assert s[0] % f == 0 and s[1] % f == 0\n",
    "        return s[0] // f, s[1] // f\n",
    "\n",
    "    def output_activation(self, x, apply_softmax):\n",
    "        if apply_softmax:\n",
    "            seg = torch.softmax(\n",
    "                x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            seg = x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1]\n",
    "        geo = torch.sigmoid(x[:, MapOrdering.GEO_TOP :]) * self.input_size[0]\n",
    "        y = torch.cat([seg, geo], dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        s = x.shape[2:]  # Original image shape HxW\n",
    "        bb5, bb4, bb3, bb2, bb1 = self.backbone(x)\n",
    "\n",
    "        y = self.up1(bb5, bb4, self.scale_shape(s, 16))\n",
    "        # up2 takes y (H/16, 256ch) and bb3 (H/8, 128ch). Upscales y to H/8. Output: H/8, 128ch.\n",
    "        y = self.up2(y, bb3, self.scale_shape(s, 8))\n",
    "        # up3 takes y (H/8, 128ch) and bb2 (H/4, 64ch). Upscales y to H/4. Output: H/4, 64ch.\n",
    "        y = self.up3(y, bb2, self.scale_shape(s, 4))\n",
    "        # up4 takes y (H/4, 64ch) and bb1 (H/2, 64ch). Upscales y to H/2. Output: H/2, 32ch.\n",
    "        y = self.up4(y, bb1, self.scale_shape(s, 2))\n",
    "\n",
    "        y = self.conv1(\n",
    "            y\n",
    "        )  # Final convolution to get NUM_MAPS channels. Output: H/2, NUM_MAPS ch.\n",
    "\n",
    "        return self.output_activation(y, apply_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f4992",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "H, W = net.input_size\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = net(test_input)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "\n",
    "nr_params = count_parameters(net)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e83a97",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a760bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_code import BoundingBox\n",
    "from my_code import IAM_Dataset\n",
    "from my_code import ImageDimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec5afd",
   "metadata": {},
   "source": [
    "First, create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf22a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment w/ dataset class\n",
    "data_path = Path.home() / 'Development/WordDetectorNN/data/train'\n",
    "dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=True,\n",
    "    transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92620d43",
   "metadata": {},
   "source": [
    "Next, access an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 578\n",
    "idx = 0\n",
    "idx = 325\n",
    "sample = dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a15b9f",
   "metadata": {},
   "source": [
    "Then, plot a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.store_element_as_image(idx, Path('test.png'), draw_bboxes=True, store_gt_encoded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bc22e",
   "metadata": {},
   "source": [
    "<h4 style=\"color: red\">~~~~ CONTINUE HERE!!! ~~~~</h4>\n",
    "\n",
    "Next, let's split the dataset into training and val datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46040a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Create indices\n",
    "indices = list(range(len(dataset)))\n",
    "split = int(0.8 * len(indices))\n",
    "\n",
    "# Shuffle indices\n",
    "np.random.seed(42)  # For reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Create subsets\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5726ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build train and val datasets\n",
    "\n",
    "# <<< CHECK THIS (alternatively, it could be a TransformSubset subclass that both subsets and subclasses)\n",
    "\n",
    "class YourImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define different transforms for train and val\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets with different transforms\n",
    "train_dataset = YourImageDataset(image_paths, labels, transform=train_transform)\n",
    "val_dataset = YourImageDataset(image_paths, labels, transform=val_transform)\n",
    "\n",
    "# Create indices and subsets\n",
    "indices = list(range(len(image_paths)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "# Create subsets - they inherit the transforms from their parent datasets\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "# >>>\n",
    "\n",
    "# TODO: Then, build dataloader fct\n",
    "\n",
    "# TODO: Then, implement the augmentations, w/ each changing at every batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48af96d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c1805",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4efebb",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- TODO: Add logging for tensorboard\n",
    "    - in particular, add hparams for parallel plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
