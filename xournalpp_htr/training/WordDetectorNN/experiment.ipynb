{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c5c9b",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Here, I want to try to re-implement the whole WordDetectorNN in a single Jupyter Notebook to keep things simple. Let's see if I get that done :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1d469",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34994a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([3], device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c54b2",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(net):\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0120",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you were using Bottleneck for other ResNet versions:\n",
    "# from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "class ModifiedResNet18(ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Initialize with BasicBlock and standard ResNet-18 layers\n",
    "        # num_classes is irrelevant here as we won't use the fc layer\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2], num_classes=1000, **kwargs)\n",
    "\n",
    "        # 1. Modify the first convolutional layer for 1-channel (grayscale) input\n",
    "        # Original resnet.conv1 is Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # We need Conv2d(1, 64, ...)\n",
    "        original_conv1 = self.conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1,\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False,\n",
    "        )  # bias is False in original ResNet conv1\n",
    "\n",
    "        # Optional: If you wanted to initialize weights similarly to torchvision:\n",
    "        # nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # However, if you load custom pretrained weights for the whole model later,\n",
    "        # this specific initialization might be overwritten.\n",
    "\n",
    "        # We don't need the final fully connected layer for feature extraction\n",
    "        del self.fc\n",
    "        # self.avgpool is also not strictly needed for the U-Net style features,\n",
    "        # but it doesn't hurt to keep it if not used. You could 'del self.avgpool' too.\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor):\n",
    "        # This is largely copied from torchvision.models.resnet.ResNet._forward_impl\n",
    "        # but modified to return intermediate features.\n",
    "\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        out1 = self.relu(x)  # Corresponds to bb1 in WordDetectorNet (before maxpool)\n",
    "        x = self.maxpool(out1)\n",
    "\n",
    "        out2 = self.layer1(x)  # Corresponds to bb2\n",
    "        out3 = self.layer2(out2)  # Corresponds to bb3\n",
    "        out4 = self.layer3(out3)  # Corresponds to bb4\n",
    "        out5 = self.layer4(out4)  # Corresponds to bb5\n",
    "\n",
    "        # WordDetectorNet expects (bb5, bb4, bb3, bb2, bb1)\n",
    "        return out5, out4, out3, out2, out1\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5009",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ModifiedResNet18()\n",
    "\n",
    "H, W = 400, 500\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = backbone(test_input)\n",
    "out5, out4, out3, out2, out1 = output\n",
    "\n",
    "print(\"Print output sizes:\")\n",
    "for o in output:\n",
    "    print(\"\\t\", o.shape)\n",
    "\n",
    "nr_params = count_parameters(backbone)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ef8e",
   "metadata": {},
   "source": [
    "Now off to the `WordDetectorNN` (for now just copied from external repo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapOrdering:\n",
    "    \"\"\"order of the maps encoding the aabbs around the words\"\"\"\n",
    "\n",
    "    SEG_WORD = 0\n",
    "    SEG_SURROUNDING = 1\n",
    "    SEG_BACKGROUND = 2\n",
    "    GEO_TOP = 3\n",
    "    GEO_BOTTOM = 4\n",
    "    GEO_LEFT = 5\n",
    "    GEO_RIGHT = 6\n",
    "    NUM_MAPS = 7\n",
    "\n",
    "\n",
    "def compute_scale_down(input_size, output_size):\n",
    "    \"\"\"compute scale down factor of neural network, given input and output size\"\"\"\n",
    "    return output_size[0] / input_size[0]\n",
    "\n",
    "\n",
    "class UpscaleAndConcatLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    take small map with cx channels\n",
    "    upscale to size of large map (s*s)\n",
    "    concat large map with cy channels and upscaled small map\n",
    "    apply conv and output map with cz channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cx, cy, cz):\n",
    "        super(UpscaleAndConcatLayer, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(cx + cy, cz, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, y, s):\n",
    "        x = F.interpolate(x, s)\n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = F.relu(self.conv(z))\n",
    "        return z\n",
    "\n",
    "\n",
    "class WordDetectorNet(torch.nn.Module):\n",
    "    input_size = (448, 448)\n",
    "    output_size = (224, 224)\n",
    "    scale_down = compute_scale_down(input_size, output_size)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WordDetectorNet, self).__init__()\n",
    "\n",
    "        # Use the modified ResNet18 for feature extraction\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        # All weights in the backbone will be randomly initialized.\n",
    "\n",
    "        self.up1 = UpscaleAndConcatLayer(512, 256, 256)  # input//16\n",
    "        self.up2 = UpscaleAndConcatLayer(256, 128, 128)  # input//8\n",
    "        self.up3 = UpscaleAndConcatLayer(128, 64, 64)  # input//4\n",
    "        self.up4 = UpscaleAndConcatLayer(64, 64, 32)  # input//2\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(32, MapOrdering.NUM_MAPS, 3, 1, padding=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_shape(s, f):\n",
    "        assert s[0] % f == 0 and s[1] % f == 0\n",
    "        return s[0] // f, s[1] // f\n",
    "\n",
    "    def output_activation(self, x, apply_softmax):\n",
    "        if apply_softmax:\n",
    "            seg = torch.softmax(\n",
    "                x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            seg = x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1]\n",
    "        geo = torch.sigmoid(x[:, MapOrdering.GEO_TOP :]) * self.input_size[0]\n",
    "        y = torch.cat([seg, geo], dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        s = x.shape[2:]  # Original image shape HxW\n",
    "        bb5, bb4, bb3, bb2, bb1 = self.backbone(x)\n",
    "\n",
    "        y = self.up1(bb5, bb4, self.scale_shape(s, 16))\n",
    "        # up2 takes y (H/16, 256ch) and bb3 (H/8, 128ch). Upscales y to H/8. Output: H/8, 128ch.\n",
    "        y = self.up2(y, bb3, self.scale_shape(s, 8))\n",
    "        # up3 takes y (H/8, 128ch) and bb2 (H/4, 64ch). Upscales y to H/4. Output: H/4, 64ch.\n",
    "        y = self.up3(y, bb2, self.scale_shape(s, 4))\n",
    "        # up4 takes y (H/4, 64ch) and bb1 (H/2, 64ch). Upscales y to H/2. Output: H/2, 32ch.\n",
    "        y = self.up4(y, bb1, self.scale_shape(s, 2))\n",
    "\n",
    "        y = self.conv1(\n",
    "            y\n",
    "        )  # Final convolution to get NUM_MAPS channels. Output: H/2, NUM_MAPS ch.\n",
    "\n",
    "        return self.output_activation(y, apply_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f4992",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "H, W = net.input_size\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = net(test_input)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "\n",
    "nr_params = count_parameters(net)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e83a97",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a760bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, x_min, y_min, x_max, y_max):\n",
    "        \"\"\"\n",
    "        Initialize a bounding box.\n",
    "        (x_min, y_min): top-left corner\n",
    "        (x_max, y_max): bottom-right corner\n",
    "        label: optional class label\n",
    "        \"\"\"\n",
    "        self.x_min = float(x_min)\n",
    "        self.y_min = float(y_min)\n",
    "        self.x_max = float(x_max)\n",
    "        self.y_max = float(y_max)\n",
    "        \n",
    "    def translate(self, dx, dy) -> \"BoundingBox\":\n",
    "        \"\"\"Translate the bounding box by (dx, dy).\"\"\"\n",
    "        bbox_translated = BoundingBox(\n",
    "            self.x_min + dx,\n",
    "            self.y_min + dy,\n",
    "            self.x_max + dx,\n",
    "            self.y_max + dy\n",
    "        )\n",
    "        return bbox_translated\n",
    "\n",
    "    def scale(self, sx, sy) -> \"BoundingBox\":\n",
    "        \"\"\"Scale the bounding box by sx and sy.\"\"\"\n",
    "        bbox_scaled = BoundingBox(\n",
    "            self.x_min * sx,\n",
    "            self.x_max * sx,\n",
    "            self.y_min * sy,\n",
    "            self.y_max * sy,\n",
    "        )\n",
    "        return bbox_scaled\n",
    "\n",
    "    def area(self):\n",
    "        \"\"\"Return the area of the bounding box.\"\"\"\n",
    "        return max(0.0, self.x_max - self.x_min) * max(0.0, self.y_max - self.y_min)\n",
    "\n",
    "    def intersect(self, other):\n",
    "        \"\"\"Return the intersection area with another bounding box.\"\"\"\n",
    "        x_min = max(self.x_min, other.x_min)\n",
    "        y_min = max(self.y_min, other.y_min)\n",
    "        x_max = min(self.x_max, other.x_max)\n",
    "        y_max = min(self.y_max, other.y_max)\n",
    "        if x_min < x_max and y_min < y_max:\n",
    "            return (x_max - x_min) * (y_max - y_min)\n",
    "        return 0.0\n",
    "\n",
    "    def iou(self, other):\n",
    "        \"\"\"Return the Intersection over Union (IoU) with another bounding box.\"\"\"\n",
    "        inter = self.intersect(other)\n",
    "        union = self.area() + other.area() - inter\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return inter / union\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BoundingBox({self.x_min}, {self.y_min}, {self.x_max}, {self.y_max})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d0449-ab3d-4f8b-847b-defdab4a318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "from typing import TypedDict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO later: Replace print with logging.\n",
    "\n",
    "class IAM_Dataset_Element(TypedDict):\n",
    "    image: np.ndarray\n",
    "    bounding_boxes: List[BoundingBox]\n",
    "    filename: str\n",
    "\n",
    "class IAM_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads, pre-processes, and caches the IAM Handwriting Database.\n",
    "\n",
    "    This class handles the entire data preparation pipeline. On the first run, it\n",
    "    processes all images and ground truth files, resizes them, and saves them\n",
    "    to a cache file for extremely fast loading on subsequent runs.\n",
    "\n",
    "    Inherits from `torch.utils.data.Dataset`, making it fully compatible with\n",
    "    PyTorch's DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    _GT_DIR_NAME = 'gt'\n",
    "    _IMG_DIR_NAME = 'img'\n",
    "    _CACHE_FILENAME = 'dataset_cache.pickle'\n",
    "    _IMG_EXT = '.png'\n",
    "    _GT_EXT = '*.xml'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path,\n",
    "        input_width: int,\n",
    "        input_height: int,\n",
    "        force_rebuild_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset. Checks for a cache file first. If it doesn't\n",
    "        exist, it builds one.\n",
    "\n",
    "        Args:\n",
    "            root_dir (Path): The root directory of the dataset, containing 'gt' and 'img' subdirectories.\n",
    "            input_size (Tuple[int, int]): The target (height, width) for the network input images.\n",
    "            loaded_img_scale (float): A factor to initially scale down images to reduce memory\n",
    "                                      usage during pre-processing. Default is 0.25.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "\n",
    "        self.img_cache: List[np.ndarray] = []\n",
    "        self.gt_cache: List[List[BoundingBox]] = []\n",
    "        self.filename_cache: List[str] = []\n",
    "\n",
    "        cache_path = self.root_dir / self._CACHE_FILENAME\n",
    "        if cache_path.exists() and not force_rebuild_cache:\n",
    "            print(f\"Loading cached data from {cache_path}...\")\n",
    "            self._load_from_cache(cache_path)\n",
    "        else:\n",
    "            print(f\"Cache not found. Building and caching data from {self.root_dir}...\")\n",
    "            self._preprocess_and_cache(cache_path)\n",
    "\n",
    "    def _load_from_cache(self, cache_path: Path):\n",
    "        \"\"\"Loads pre-processed data from a pickle file.\"\"\"\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            self.img_cache, self.gt_cache, self.filename_cache = pickle.load(f)\n",
    "\n",
    "    def _preprocess_and_cache(self, cache_path: Path):\n",
    "        \"\"\"Finds, processes, and caches all data samples.\"\"\"\n",
    "        gt_dir = self.root_dir / self._GT_DIR_NAME\n",
    "        img_dir = self.root_dir / self._IMG_DIR_NAME\n",
    "\n",
    "        fn_gts = sorted(gt_dir.glob(self._GT_EXT))\n",
    "        print(f\"Found {len(fn_gts)} ground truth files. Processing...\")\n",
    "\n",
    "        for fn_gt in tqdm(fn_gts, desc=\"Preprocessing IAM Dataset\"):\n",
    "            fn_img = img_dir / (fn_gt.stem + self._IMG_EXT)\n",
    "            if not fn_img.exists():\n",
    "                continue\n",
    "\n",
    "            # Load image and GT\n",
    "            img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
    "            gt = self._parse_gt(fn_gt)\n",
    "\n",
    "            # Pre-processing pipeline\n",
    "            raise NotImplementedError\n",
    "            # img, gt = self._crop_page_to_content(img, gt)\n",
    "            # img, gt = self._adjust_to_size(img, gt)\n",
    "\n",
    "            self.img_cache.append(img)\n",
    "            self.gt_cache.append(gt)\n",
    "            self.filename_cache.append(fn_gt.stem)\n",
    "\n",
    "        print(f\"Preprocessing complete. Saving cache to {cache_path}...\")\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump([self.img_cache, self.gt_cache, self.filename_cache], f)\n",
    "        print(\"Cache saved successfully.\")\n",
    "\n",
    "\n",
    "    def _parse_gt(self, fn_gt: Path) -> List[BoundingBox]:\n",
    "        \"\"\"Parses an XML ground truth file to get word bounding boxes.\"\"\"\n",
    "        tree = ET.parse(fn_gt)\n",
    "        root = tree.getroot()\n",
    "        aabbs = []\n",
    "\n",
    "        for line in root.findall(\"./handwritten-part/line\"):\n",
    "            for word in line.findall('./word'):\n",
    "                x_min, x_max, y_min, y_max = float('inf'), 0, float('inf'), 0\n",
    "                components = word.findall('./cmp')\n",
    "                if not components:\n",
    "                    continue\n",
    "\n",
    "                for cmp in components:\n",
    "                    x = float(cmp.attrib['x'])\n",
    "                    y = float(cmp.attrib['y'])\n",
    "                    w = float(cmp.attrib['width'])\n",
    "                    h = float(cmp.attrib['height'])\n",
    "                    x_min = min(x_min, x)\n",
    "                    x_max = max(x_max, x + w)\n",
    "                    y_min = min(y_min, y)\n",
    "                    y_max = max(y_max, y + h)\n",
    "                \n",
    "                # Scale coordinates to match the initially scaled image\n",
    "                aabb = BoundingBox(x_min, x_max, y_min, y_max)\n",
    "                aabbs.append(aabb)\n",
    "        return aabbs\n",
    "\n",
    "    def _crop_page_to_content(self, img: np.ndarray, gt: List[BoundingBox]) -> Tuple[np.ndarray, List[BoundingBox]]:\n",
    "        \"\"\"Crops the image to the bounding box containing all words.\"\"\"\n",
    "        x_min = min(aabb.x_min for aabb in gt)\n",
    "        x_max = max(aabb.x_max for aabb in gt)\n",
    "        y_min = min(aabb.y_min for aabb in gt)\n",
    "        y_max = max(aabb.y_max for aabb in gt)\n",
    "\n",
    "        gt_crop = [aabb.translate(-x_min, -y_min) for aabb in gt]\n",
    "        img_crop = img[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "        return img_crop, gt_crop\n",
    "\n",
    "    def _adjust_to_size(self, img: np.ndarray, gt: List[BoundingBox]) -> Tuple[np.ndarray, List[BoundingBox]]:\n",
    "        \"\"\"Resizes the image and AABBs to the final network input size.\"\"\"\n",
    "        h, w = img.shape\n",
    "        fx = self.input_width / w\n",
    "        fy = self.input_height / h\n",
    "        \n",
    "        gt_resized = [aabb.scale(fx, fy) for aabb in gt]\n",
    "        img_resized = cv2.resize(img, dsize=(self.input_width, self.input_height)) # cv2 uses (w, h)\n",
    "        return img_resized, gt_resized\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.img_cache)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> IAM_Dataset_Element:\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - The pre-processed image as a NumPy array.\n",
    "            - A list of AABB objects for the ground truth words.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'image': self.img_cache[idx],\n",
    "            'bounding_boxes': self.gt_cache[idx],\n",
    "            'filename': self.filename_cache[idx],\n",
    "        }\n",
    "    \n",
    "    def store_element_as_image(self, idx: int, output_path: Path, draw_bboxes: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Saves a dataset element as an image with bounding boxes drawn on it.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the dataset element to save.\n",
    "            output_path (Path): The path where the image should be saved.\n",
    "        \"\"\"\n",
    "        # Get the element\n",
    "        element = self[idx]\n",
    "        img = element['image'].copy()  # Copy to avoid modifying the cached image\n",
    "        bboxes = element['bounding_boxes']\n",
    "        \n",
    "        # Convert grayscale to BGR for colored bounding boxes\n",
    "        img_color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        if draw_bboxes:\n",
    "            for bbox in bboxes:\n",
    "                # Convert float coordinates to integers\n",
    "                x_min = int(bbox.x_min)\n",
    "                y_min = int(bbox.y_min)\n",
    "                x_max = int(bbox.x_max)\n",
    "                y_max = int(bbox.y_max)\n",
    "                \n",
    "                # Draw rectangle (green color, thickness=2)\n",
    "                cv2.rectangle(img_color, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        \n",
    "        # Save the image\n",
    "        cv2.imwrite(str(output_path), img_color)\n",
    "\n",
    "\n",
    "# TODO: Add plotting functions to visualize the dataset samples and their ground truth annotations. In particular useful given many transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf22a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment w/ dataset class\n",
    "data_path = Path.home() / 'Development/WordDetectorNN/data/train'\n",
    "dataset = IAM_Dataset(root_dir=data_path, input_width=448, input_height=448, force_rebuild_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c92acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix dataset!\n",
    "dataset[43][1][0] # -> the 0.0 is suspicous\n",
    "dataset[349][1][0] # -> the 0.0 is suspicous\n",
    "# -> it seems like the cropping AND bounding box extraction process is the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.store_element_as_image(578, 'test.png', draw_bboxes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[578]['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48af96d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c1805",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4efebb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordDetectorNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
