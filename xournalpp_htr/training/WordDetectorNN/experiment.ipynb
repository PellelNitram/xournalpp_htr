{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c5c9b",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Here, I want to try to re-implement the whole WordDetectorNN in a single Jupyter Notebook to keep things simple. Let's see if I get that done :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from my_code import IAM_Dataset\n",
    "from my_code import ImageDimensions\n",
    "from my_code import custom_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1d469",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34994a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([3], device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559f3ce",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Here, I note down how I build the project to remind myself and others in the future. Here we go:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IAM Folder] --> B[Train Dataset = Dtr]\n",
    "    A --> C[Val Dataset = Dval]\n",
    "    B --> D[Dtr w transform, img&aabb = Dtr']\n",
    "    C --> E[Dval w transform, img&aabb = Dval']\n",
    "    D --> F[Train DataLoader] \n",
    "    E --> G[Val DataLoader] \n",
    "    E --> H[no transform except normalisation]\n",
    "    D --> I[geometric & photo]\n",
    "```\n",
    "\n",
    "- transform in Dataset\n",
    "- encode in collate of DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2961b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc41c4",
   "metadata": {},
   "source": [
    "First, create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment w/ dataset class\n",
    "data_path = Path.home() / 'Development/WordDetectorNN/data/train'\n",
    "dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=True,\n",
    "    transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a61a88",
   "metadata": {},
   "source": [
    "Next, access an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 578\n",
    "idx = 0\n",
    "idx = 325\n",
    "sample = dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8f404",
   "metadata": {},
   "source": [
    "Then, plot a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.store_element_as_image(idx, Path('test.png'), draw_bboxes=True, store_gt_encoded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1bab0c",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into training and val datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way to create the train and val datasets seems convoluted but is necessary to ensure\n",
    "# that train and val get only their transforms. I know that it could be implemented more efficiently\n",
    "# but that's not necessary give the small dataset.\n",
    "#\n",
    "# An alternative way to implement it is to build a TransformSubset which not only Subset's but also\n",
    "# applies a separate transform.\n",
    "#\n",
    "# Note that it is not a good idea to hardcode these transforms b/c one might want to use the plain dataset,\n",
    "# even if only for inspection\n",
    "\n",
    "# Create datasets with different transforms\n",
    "train_transform = None\n",
    "val_transform = None\n",
    "# TODO: ^ Implement the augmentations, w/ each changing at every batch\n",
    "\n",
    "train_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=False,\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    # input_size=ImageDimensions(width=640, height=448),\n",
    "    input_size=ImageDimensions(width=400, height=600),\n",
    "    output_size=ImageDimensions(width=200, height=300),\n",
    "    force_rebuild_cache=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "percent_train_data = 80\n",
    "\n",
    "assert len(train_dataset) == len(val_dataset)\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split = int(percent_train_data / 100 * len(indices))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_filenames = [sample['filename'] for sample in train_subset]\n",
    "val_filenames = [sample['filename'] for sample in val_subset]\n",
    "# Check that no train samples are in val\n",
    "assert len(set(train_filenames + val_filenames)) == len(train_filenames) + len(val_filenames)\n",
    "\n",
    "assert len(dataset) == len(train_subset) + len(val_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57d696",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data_loader = True\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27404d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176d44a",
   "metadata": {},
   "source": [
    "Check lenghts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_train), len(train_subset), len(train_subset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360025b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_val), len(val_subset), len(val_subset) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a53d3",
   "metadata": {},
   "source": [
    "Load a single batch for testing & inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = next(iter(dataloader_train))\n",
    "batch_val = next(iter(dataloader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fff7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train.keys(), batch_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe460c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train['images'].shape, batch_val['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train['gt_encoded'].shape, batch_val['gt_encoded'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af839ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_train['bounding_boxes']), len(batch_val['bounding_boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1becb",
   "metadata": {},
   "source": [
    "Iterate through whole dataloader once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader_train:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d444e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader_val:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0120",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you were using Bottleneck for other ResNet versions:\n",
    "# from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "class ModifiedResNet18(ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Initialize with BasicBlock and standard ResNet-18 layers\n",
    "        # num_classes is irrelevant here as we won't use the fc layer\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2], num_classes=1000, **kwargs)\n",
    "\n",
    "        # 1. Modify the first convolutional layer for 1-channel (grayscale) input\n",
    "        # Original resnet.conv1 is Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # We need Conv2d(1, 64, ...)\n",
    "        original_conv1 = self.conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1,\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False,\n",
    "        )  # bias is False in original ResNet conv1\n",
    "\n",
    "        # Optional: If you wanted to initialize weights similarly to torchvision:\n",
    "        # nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # However, if you load custom pretrained weights for the whole model later,\n",
    "        # this specific initialization might be overwritten.\n",
    "\n",
    "        # We don't need the final fully connected layer for feature extraction\n",
    "        del self.fc\n",
    "        # self.avgpool is also not strictly needed for the U-Net style features,\n",
    "        # but it doesn't hurt to keep it if not used. You could 'del self.avgpool' too.\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor):\n",
    "        # This is largely copied from torchvision.models.resnet.ResNet._forward_impl\n",
    "        # but modified to return intermediate features.\n",
    "\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        out1 = self.relu(x)  # Corresponds to bb1 in WordDetectorNet (before maxpool)\n",
    "        x = self.maxpool(out1)\n",
    "\n",
    "        out2 = self.layer1(x)  # Corresponds to bb2\n",
    "        out3 = self.layer2(out2)  # Corresponds to bb3\n",
    "        out4 = self.layer3(out3)  # Corresponds to bb4\n",
    "        out5 = self.layer4(out4)  # Corresponds to bb5\n",
    "\n",
    "        # WordDetectorNet expects (bb5, bb4, bb3, bb2, bb1)\n",
    "        return out5, out4, out3, out2, out1\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5009",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ModifiedResNet18()\n",
    "\n",
    "H, W = 400, 500\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = backbone(test_input)\n",
    "out5, out4, out3, out2, out1 = output\n",
    "\n",
    "print(\"Print output sizes:\")\n",
    "for o in output:\n",
    "    print(\"\\t\", o.shape)\n",
    "\n",
    "nr_params = count_parameters(backbone)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753ef8e",
   "metadata": {},
   "source": [
    "Now off to the `WordDetectorNN` (for now just copied from external repo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapOrdering:\n",
    "    \"\"\"order of the maps encoding the aabbs around the words\"\"\"\n",
    "\n",
    "    SEG_WORD = 0\n",
    "    SEG_SURROUNDING = 1\n",
    "    SEG_BACKGROUND = 2\n",
    "    GEO_TOP = 3\n",
    "    GEO_BOTTOM = 4\n",
    "    GEO_LEFT = 5\n",
    "    GEO_RIGHT = 6\n",
    "    NUM_MAPS = 7\n",
    "\n",
    "\n",
    "def compute_scale_down(input_size, output_size):\n",
    "    \"\"\"compute scale down factor of neural network, given input and output size\"\"\"\n",
    "    return output_size[0] / input_size[0]\n",
    "\n",
    "\n",
    "class UpscaleAndConcatLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    take small map with cx channels\n",
    "    upscale to size of large map (s*s)\n",
    "    concat large map with cy channels and upscaled small map\n",
    "    apply conv and output map with cz channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cx, cy, cz):\n",
    "        super(UpscaleAndConcatLayer, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(cx + cy, cz, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, y, s):\n",
    "        x = F.interpolate(x, s)\n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = F.relu(self.conv(z))\n",
    "        return z\n",
    "\n",
    "\n",
    "class WordDetectorNet(torch.nn.Module):\n",
    "    input_size = (448, 448)\n",
    "    output_size = (224, 224)\n",
    "    scale_down = compute_scale_down(input_size, output_size)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WordDetectorNet, self).__init__()\n",
    "\n",
    "        # Use the modified ResNet18 for feature extraction\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        # All weights in the backbone will be randomly initialized.\n",
    "\n",
    "        self.up1 = UpscaleAndConcatLayer(512, 256, 256)  # input//16\n",
    "        self.up2 = UpscaleAndConcatLayer(256, 128, 128)  # input//8\n",
    "        self.up3 = UpscaleAndConcatLayer(128, 64, 64)  # input//4\n",
    "        self.up4 = UpscaleAndConcatLayer(64, 64, 32)  # input//2\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(32, MapOrdering.NUM_MAPS, 3, 1, padding=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_shape(s, f):\n",
    "        assert s[0] % f == 0 and s[1] % f == 0\n",
    "        return s[0] // f, s[1] // f\n",
    "\n",
    "    def output_activation(self, x, apply_softmax):\n",
    "        if apply_softmax:\n",
    "            seg = torch.softmax(\n",
    "                x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            seg = x[:, MapOrdering.SEG_WORD : MapOrdering.SEG_BACKGROUND + 1]\n",
    "        geo = torch.sigmoid(x[:, MapOrdering.GEO_TOP :]) * self.input_size[0]\n",
    "        y = torch.cat([seg, geo], dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        s = x.shape[2:]  # Original image shape HxW\n",
    "        bb5, bb4, bb3, bb2, bb1 = self.backbone(x)\n",
    "\n",
    "        y = self.up1(bb5, bb4, self.scale_shape(s, 16))\n",
    "        # up2 takes y (H/16, 256ch) and bb3 (H/8, 128ch). Upscales y to H/8. Output: H/8, 128ch.\n",
    "        y = self.up2(y, bb3, self.scale_shape(s, 8))\n",
    "        # up3 takes y (H/8, 128ch) and bb2 (H/4, 64ch). Upscales y to H/4. Output: H/4, 64ch.\n",
    "        y = self.up3(y, bb2, self.scale_shape(s, 4))\n",
    "        # up4 takes y (H/4, 64ch) and bb1 (H/2, 64ch). Upscales y to H/2. Output: H/2, 32ch.\n",
    "        y = self.up4(y, bb1, self.scale_shape(s, 2))\n",
    "\n",
    "        y = self.conv1(\n",
    "            y\n",
    "        )  # Final convolution to get NUM_MAPS channels. Output: H/2, NUM_MAPS ch.\n",
    "\n",
    "        return self.output_activation(y, apply_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f4992",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WordDetectorNet()\n",
    "\n",
    "H, W = net.input_size\n",
    "test_input = torch.randn((1, 1, H, W))\n",
    "\n",
    "output = net(test_input)\n",
    "\n",
    "print(\"Print output sizes:\", output.shape)\n",
    "\n",
    "nr_params = count_parameters(net)\n",
    "print(f\"Total params: {nr_params['total_params']}\")\n",
    "print(f\"Trainable params: {nr_params['trainable_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c1805",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4efebb",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- TODO: Add logging for tensorboard\n",
    "    - in particular, add hparams for parallel plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
