{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from my_code import BoundingBox\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from my_code import WordDetectorNet\n",
    "import cv2\n",
    "\n",
    "\n",
    "from my_code import normalize_image_transform\n",
    "from my_code import ImageDimensions\n",
    "from my_code import IAM_Dataset\n",
    "from my_code import custom_collate_fn\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from my_code import MapOrdering\n",
    "\n",
    "from my_code import decode, fg_by_cc, BoundingBox, cluster_aabbs, draw_bboxes_on_image\n",
    "\n",
    "\n",
    "def run_image_through_network(\n",
    "        image_grayscale: np.ndarray,\n",
    "        model_path: Path=Path('best_model.pth'),\n",
    "    ) -> List[BoundingBox]:\n",
    "\n",
    "    # ================\n",
    "    # Configure system\n",
    "    # ================\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ==========\n",
    "    # Load model\n",
    "    # ==========\n",
    "\n",
    "    model = WordDetectorNet()  # instantiate your model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ==============\n",
    "    # Pre processing\n",
    "    # ==============\n",
    "\n",
    "    image_gray_rescaled = cv2.resize(image_grayscale, WordDetectorNet.input_size)\n",
    "\n",
    "    image_grayscale_transformed, _ = normalize_image_transform(image_gray_rescaled, None) # Only works w/ current transformation setup\n",
    "\n",
    "    image_grayscale_transformed = image_grayscale_transformed.astype(np.float32)\n",
    "    \n",
    "    image_grayscale_transformed = torch.from_numpy(image_grayscale_transformed[None, None, :, :]).to(device)\n",
    "\n",
    "    # =========\n",
    "    # Inference\n",
    "    # =========\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_image = model(image_grayscale_transformed, apply_softmax=True)\n",
    "\n",
    "    assert output_image[:, MapOrdering.SEG_WORD:MapOrdering.SEG_BACKGROUND+1, :, :].min() >= 0.0\n",
    "    assert output_image[:, MapOrdering.SEG_WORD:MapOrdering.SEG_BACKGROUND+1, :, :].max() <= 1.0\n",
    "\n",
    "    output_image = output_image.to('cpu').numpy()\n",
    "\n",
    "    output_image = output_image[0, :, :, :]\n",
    "\n",
    "    # ===============\n",
    "    # Post processing\n",
    "    # ===============\n",
    "\n",
    "    decoded_aabbs = decode(\n",
    "        output_image,\n",
    "        scale=WordDetectorNet.input_size[0] / WordDetectorNet.output_size[0],\n",
    "        comp_fg=fg_by_cc(thres=0.5, max_num=1000),\n",
    "    )\n",
    "    model_input_image = image_grayscale_transformed[0, 0, :, :].to('cpu').numpy()\n",
    "    h, w = model_input_image.shape\n",
    "    aabbs = [aabb.clip(BoundingBox(0, 0, w - 1, h - 1)) for aabb in decoded_aabbs]  # bounding box must be inside input img\n",
    "    clustered_aabbs = cluster_aabbs(aabbs)\n",
    "\n",
    "    return {\n",
    "        'aabbs': clustered_aabbs,\n",
    "        'model_input_image': model_input_image,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_path = Path('cvl.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6216a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(str(example_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f'Original Image @ {cv2.cvtColor(image, cv2.COLOR_BGR2RGB).shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gray_image, cmap='gray')\n",
    "axes[1].set_title(f'Grayscale Image @ {gray_image.shape}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "fig.suptitle('input vs grayscaled image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_image_through_network(\n",
    "    image_grayscale=gray_image,\n",
    "    model_path=Path('best_model.pth'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = draw_bboxes_on_image(result['model_input_image'], result['aabbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e75265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "inp = result['model_input_image']\n",
    "axes[0].imshow(inp, cmap='gray')\n",
    "axes[0].set_title(f'NN input @ {inp.shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "pred = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(pred, cmap='gray')\n",
    "axes[1].set_title(f'NN prediction @ {pred.shape}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move fct into `my_code` module\n",
    "# TODO: Reformat into old image size -> scale up only bouding boxes and overlay on originally loaded image; maybe also on rescaled returned image, just to make sure\n",
    "# TODO: Put into gradio app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
