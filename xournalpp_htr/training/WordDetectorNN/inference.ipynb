{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from my_code import BoundingBox\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from my_code import WordDetectorNet\n",
    "import cv2\n",
    "\n",
    "\n",
    "def run_image_through_network(image_path: Path, model_path: Path=Path('best_model.pth')) -> List[BoundingBox]:\n",
    "\n",
    "    # ========\n",
    "    # Settings\n",
    "    # ========\n",
    "\n",
    "    model_path = Path('best_model.pth') # later, replace w/ cli argument\n",
    "\n",
    "    # ================\n",
    "    # Configure system\n",
    "    # ================\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ==========\n",
    "    # Load model\n",
    "    # ==========\n",
    "\n",
    "    model = WordDetectorNet()  # instantiate your model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_path = Path('cvl.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc94aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = Image.open('cvl.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6216a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(str(example_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24430ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f'Original Image @ {cv2.cvtColor(image, cv2.COLOR_BGR2RGB).shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gray_image, cmap='gray')\n",
    "axes[1].set_title(f'Grayscale Image @ {gray_image.shape}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c01a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = WordDetectorNet.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244141f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_gray_image = cv2.resize(gray_image, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02510d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(gray_image, cmap='gray')\n",
    "axes[0].set_title(f'Grayscale Image @ {gray_image.shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(resized_gray_image, cmap='gray')\n",
    "axes[1].set_title(f'Grayscale Image Resized @ {resized_gray_image.shape}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea964a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_code import normalize_image_transform\n",
    "from my_code import ImageDimensions\n",
    "from my_code import IAM_Dataset\n",
    "from my_code import custom_collate_fn\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98cac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "val_epoch = 5\n",
    "val_epoch = 1\n",
    "\n",
    "summary_writer_path = Path.home() / 'summary_writer_path'\n",
    "\n",
    "epoch_max = 100 # Simulate full training\n",
    "# epoch_max = 10\n",
    "# epoch_max = 0\n",
    "epoch_max = 3\n",
    "# epoch_max = 10000\n",
    "\n",
    "patience_max = 50\n",
    "\n",
    "# Dataset settings\n",
    "data_path = Path.home() / 'Development/WordDetectorNN/data/train'\n",
    "percent_train_data = 80\n",
    "input_size = ImageDimensions(width=448, height=448)\n",
    "output_size = ImageDimensions(width=224, height=224)\n",
    "\n",
    "# Dataloader settings\n",
    "shuffle_data_loader = True\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I copied the code from above\n",
    "\n",
    "# -- datasets --\n",
    "\n",
    "# Create datasets with different transforms\n",
    "train_transform = normalize_image_transform\n",
    "val_transform = normalize_image_transform\n",
    "# TODO: ^ Implement the augmentations, w/ each changing at every batch\n",
    "\n",
    "train_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    force_rebuild_cache=True,\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = IAM_Dataset(\n",
    "    root_dir=data_path,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    force_rebuild_cache=True,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "assert len(train_dataset) == len(val_dataset)\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split = int(percent_train_data / 100 * len(indices))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_filenames = [sample['filename'] for sample in train_subset]\n",
    "val_filenames = [sample['filename'] for sample in val_subset]\n",
    "# Check that no train samples are in val\n",
    "assert len(set(train_filenames + val_filenames)) == len(train_filenames) + len(val_filenames)\n",
    "\n",
    "# assert len(dataset) == len(train_subset) + len(val_subset)\n",
    "\n",
    "# -- dataloaders --\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_data_loader,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # no need to shuffle validation data and otherwise images break\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=custom_collate_fn,  # or custom_collate_fn_with_padding\n",
    "    pin_memory=True  # For faster GPU transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d73765",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch_sample['images'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images), images.shape, images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.from_numpy(resized_gray_image[None, None, :, :]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_image), input_image.shape, input_image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_transformed, _ = normalize_image_transform(input_image, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee69b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_image_transformed), input_image_transformed.shape, input_image_transformed.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_image = model(input_image_transformed, apply_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_code import MapOrdering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.shape\n",
    "\n",
    "assert output_image[:, MapOrdering.SEG_WORD:MapOrdering.SEG_BACKGROUND+1, :, :].min() >= 0.0\n",
    "assert output_image[:, MapOrdering.SEG_WORD:MapOrdering.SEG_BACKGROUND+1, :, :].max() <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e903ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.shape[0]\n",
    "\n",
    "output_image = output_image.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_code import decode, fg_by_cc, BoundingBox, cluster_aabbs, draw_bboxes_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = ImageDimensions(width=448, height=448)\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_element_in_batch = 0\n",
    "y_element = output_image[i_element_in_batch, :, :, :]\n",
    "decoded_aabbs = decode(y_element, scale=input_size.width / output_size.width, comp_fg=fg_by_cc(thres=0.5, max_num=1000))\n",
    "img_np = input_image_transformed[i_element_in_batch, 0, :, :].to('cpu').numpy()\n",
    "h, w = img_np.shape\n",
    "aabbs = [aabb.clip(BoundingBox(0, 0, w - 1, h - 1)) for aabb in decoded_aabbs]  # bounding box must be inside img\n",
    "clustered_aabbs = cluster_aabbs(aabbs)\n",
    "print(len(clustered_aabbs))\n",
    "vis = draw_bboxes_on_image(img_np, clustered_aabbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e75265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "inp = input_image_transformed.to('cpu')[0, 0, :, :]\n",
    "axes[0].imshow(inp, cmap='gray')\n",
    "axes[0].set_title(f'NN input @ {inp.shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "pred = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(pred, cmap='gray')\n",
    "axes[1].set_title(f'NN prediction @ {pred.shape}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1339d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
