{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Xournal++ HTR","text":"<p>Developing handwritten text recognition for Xournal++.</p> <p>Your contributions are greatly appreciated!</p>"},{"location":"#xournal-htr-in-90-seconds","title":"Xournal++ HTR in 90 seconds","text":"(Click here to get to video on YouTube.)"},{"location":"#demo","title":"Demo","text":"<p>Try out Xournal++ HTR without any installation by using our online demo! \ud83e\udd73</p>"},{"location":"#why-handwritten-text-recognition-for-xournal","title":"Why Handwritten Text Recognition for Xournal++?","text":"<p>A key benefit of digital note-taking is searchability, which digital handwritten notes lack without handwritten text recognition (HTR). While many commercial apps offer this feature, no open-source, privacy-focused handwriting app does - until now.</p> <p>The Xournal++ HTR project aims to bring on-device handwriting recognition to Xournal++, a leading open-source note-taking platform. This will make handwritten notes searchable while ensuring user privacy through local data processing.</p>"},{"location":"#content-of-these-websites","title":"Content of these websites","text":"<p>These websites document Xournal++ HTR. In the navigation bar, you can find instructions on how to install the project, use the project and more advanced topics like how you can contribute code and own models. In the future, many of the documents will come with small videos to get you going quicker.</p>"},{"location":"#cite","title":"Cite","text":"<p>If you are using Xournal++ HTR for your research, I'd appreciate if you could cite it. Use:</p> <pre><code>@software{Lellep_Xournalpp_HTR,\n  author = {Lellep, Martin},\n  title = {xournalpp_htr},\n  url = {https://github.com/PellelNitram/xournalpp_htr},\n  license = {GPL-2.0},\n}\n</code></pre> <p>(Also please consider starring the project on GitHub.)</p>"},{"location":"contributing/","title":"Contributing","text":"<p>There are multiple ways to contribute to this project. Below, those ways are explained alongside information on how to best contribute from a codebase point of view.</p> <p>Really, we greatly appreciate any help!</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to contribute","text":""},{"location":"contributing/#reach-out","title":"Reach out","text":"<p>If you have questions about how to best contribute or the slightest interest in contributing, then feel free to reach out to me at any time :-).</p>"},{"location":"contributing/#issues-on-github","title":"Issues on Github","text":"<p>A great way to help out with this project is to check open issues on Github and to try to work on them.</p> <p>If you need support with those, then please reach out to - we're very happy to help!</p>"},{"location":"contributing/#things-to-consider-when-contributing","title":"Things to consider when contributing","text":""},{"location":"contributing/#branching-strategy","title":"Branching strategy","text":"<p>The following branching strategy is used to keep the <code>master</code> branch stable and allow for experimentation: <code>master</code> &gt; <code>dev</code> &gt; <code>feature branches</code>. This branching strategy is shown in the following visualisation and then explained in more detail in the next paragraph:</p> <pre><code>%%{init:{  \"gitGraph\":{ \"mainBranchName\":\"master\" }}}%%\ngitGraph\n    commit\n    commit\n    branch dev\n    commit\n    checkout dev\n    commit\n    commit\n    branch feature/awesome_new_feature\n    commit\n    checkout feature/awesome_new_feature\n    commit\n    commit\n    commit\n    checkout dev\n    merge feature/awesome_new_feature\n    commit\n    commit\n    checkout master\n    merge dev\n    commit\n    commit</code></pre> <p>In more details, this repository adheres to the following git branching strategy: The <code>master</code> branch remains stable and delivers a functioning product. The <code>dev</code> branch consists of all code that will be merged to <code>master</code> eventually where the corresponding features are developed in individual feature branches; the above visualisation shows an example feature branch called <code>feature/awesome_new_feature</code> that works on a feature called <code>awesome_new_feature</code>.</p> <p>Given this structure, please implement new features as feature branches and rebase them onto the <code>dev</code> branch prior to sending a pull request to <code>dev</code>.</p> <p>Note: The Github Actions CI/CD pipeline runs on the branches <code>master</code> and <code>dev</code>.</p>"},{"location":"contributing/#code-quality","title":"Code quality","text":"<p>We try to keep up code quality as high as practically possible. For that reason, the following steps are implemented:</p> <ul> <li>Testing. Xournal++ HTR uses <code>pytest</code> for unit, regression and integration tests.</li> <li>Linting. Xournal++ HTR uses <code>ruff</code> for linting and code best practises. <code>ruff</code> is implemented as git pre-commit hook. Since <code>ruff</code> as pre-commit hook is configured externally with <code>pyproject.toml</code>, you can use the same settings in your IDE (e.g. VSCode) if you wish to speed up the process.</li> <li>Formatting. Xournal++ HTR uses <code>ruff-format</code> for consistent code formatting. <code>ruff-format</code> is implemented as git pre-commit hook. Since <code>ruff-format</code> as pre-commit hook is configured externally with <code>pyproject.toml</code>, you can use the same settings in your IDE if you wish to speed up the process.</li> </ul>"},{"location":"demo_setup/","title":"Demo Setup","text":"<p>Our online demo allows users to experiment with Xournal++ HTR without any installation and thereby lowers the entry barrier.</p> <p>We use a Hugging Face Docker Space setup to deliver the online demo. The deployment is automated using Github Actions, see this workflow.</p> <p>This page outlines how to set up and develop the demo locally, ensuring compatibility with Hugging Face Docker Space deployment.</p>"},{"location":"demo_setup/#local-development-using-docker","title":"Local development using Docker","text":"<ol> <li>Build the Docker image from the repository root folder: <code>docker build -t xournalpp_htr .</code></li> <li>Run Docker image: <code>docker run -d -p 7860:7860 xournalpp_htr</code></li> <li>Run Docker image for interactive development<ul> <li>Start docker container: <code>docker run -it -p 7860:7860 -v $(pwd):/temp_code_mount --entrypoint bash xournalpp_htr</code></li> <li>Call Python code inside the container: <code>python /temp_code_mount/scripts/demo.py</code></li> </ul> </li> </ol> <p>Generally, tidy up Docker caches with <code>docker system prune</code> if your system is full.</p>"},{"location":"demo_setup/#production-deployment-using-github-actions","title":"Production deployment using Github Actions","text":"<p>Once code has been pushed to the <code>master</code> branch, it is picked up by CI/CD in the form of Github Actions (see code here) and the code is automatically deployed to a Hugging Face Docker Space here.</p>"},{"location":"demo_setup/#environment-variables","title":"Environment variables","text":"<p>The demo needs a number of environment variables to work correctly, see below and in the <code>.env.example</code> file: </p> <pre><code>DEMO=1\nSB_URL=\"https://&lt;add here&gt;.supabase.co\"\nSB_KEY=\"&lt;add here&gt;\"\nSB_BUCKET_NAME=\"xournalpp_htr_hf_space\"\nSB_SCHEMA_NAME=\"public\"\nSB_TABLE_NAME=\"xournalpp_htr_hf_space_events\"\n</code></pre> <p>Here, demo mode should be disabled for the production environment, i.e. <code>DEMO=0</code>.</p>"},{"location":"demo_setup/#supabase-setup","title":"Supabase setup","text":"<p>Supabase stores both analytics data and donated data samples.</p> <p>Create the events table for analytics:</p> <pre><code>create table public.xournalpp_htr_hf_space_events (\n  id bigserial primary key,\n  timestamp timestamptz not null,\n  demo boolean not null,\n  session_id text not null,\n  donate_data bool not null,\n  interaction text not null\n);\n</code></pre> <p>Create bucket with following name to store donated data samples:</p> <pre><code>xournalpp_htr_hf_space\n</code></pre>"},{"location":"developer_guide/","title":"Developer Guide","text":""},{"location":"developer_guide/#project-design","title":"Project design","text":"<p>The design of Xournal++ HTR tries to bridge the gap between both delivering a production ready product and allowing contributors to experiment with new algorithms.</p> <p>The project design involves a Lua plugin and a Python backend, see the following figure. First, the production ready product is delivered by means of an Xournal++ plugin. The plugin is fully integrated in Xournal++ and calls a Python backend that performs the actual transcription. The Python backend allows selection of various recognition models and is thereby fully extendable with new models.</p> <pre><code>sequenceDiagram\n    User in Xpp--&gt;&gt;Xpp HTR Plugin: starts transcription process using currently open file\n    Xpp HTR Plugin --&gt;&gt; Xpp HTR Lua Plugin: calls\n    Xpp HTR Lua Plugin --&gt;&gt;Xpp HTR Python Backend: constructs command using CLI\n    Xpp HTR Python Backend --&gt;&gt; Xpp HTR Python Backend: Does OCR &amp; stores PDF\n    Xpp HTR Python Backend--&gt;&gt;User in Xpp: Gives back control to UI</code></pre> <p>Developing a usable HTR systems requires experimentation. The project structure is set up to accommodate this need. Note that ideas on improved project structures are appreciated.</p> <p>The experimentation is carried out in terms of \"concepts\". Each concept explores a different approach to HTR and possibly improves over previous concepts, but not necessarily to allow for freedom in risky experiments. Concept 1 is already implemented and uses a computer vision approach that is explained below.</p> <p>Future concepts might explore:</p> <ul> <li>Retrain computer vision models from concept 1 using native online data representation of Xournal++</li> <li>Use sequence-to-sequence models to take advantage of native online data representation of Xournal++; e.g. use OnlineHTR</li> <li>Use data augmentation to increase effective size of training data</li> <li>Use of language models to correct for spelling mistakes</li> </ul>"},{"location":"developer_guide/#concept-1","title":"Concept 1","text":"<p>This concept uses computer vision based algorithms to first detect words on a page and then to read those words.</p> <p>The following shows a video demo on YouTube using real-life handwriting data from a Xournal file:</p> <p></p> <p>Despite not being perfect, the main take away is that the performance is surprisingly good given that the underlying algorithm has not been optimised for Xournal++ data at all.</p> <p>The performance is sufficiently good to be useful for the Xournal++ user base.</p> <p>Feel free to play around with the demo yourself using this code after installing this project. The \"concept 1\" is also what is currently used in the plugin and shown in the 90 seconds demo.</p> <p>Next steps to improve the performance of the handwritten text recognition even further could be:</p> <ul> <li>Re-train the algorithm on Xournal++ specific data, while potentially using data augmentation.</li> <li>Use language model to improve text encoding.</li> <li>Use sequence-to-sequence algorithm that makes use of Xournal++'s data format. This translates into using online HTR algorithms.</li> </ul> <p>I would like to acknowledge Harald Scheidl in this concept as he wrote the underlying algorithms and made them easily usable through his HTRPipeline repository - after all I just feed his algorithm Xournal++ data in concept 1. Go check out his great content!</p>"},{"location":"funding/","title":"Funding","text":"<p>This project is mostly a solo project and I love to work on it (please contribute, if you want to - happy to help along the way!).</p> <p>However, it is both a large time commitment and requires compute resources for training models.</p> <p>If you think this project is valuable and want to express your gratitute, then please feel free to buy me a virtual coffee here :-).</p> <p>Thanks!!</p>"},{"location":"installation_developer/","title":"Development installation","text":"<ol> <li>Perform the same installation steps as described in the user installation manual.</li> <li>Then, install developer dependencies: <code>pip install -r requirements_training.txt</code>.</li> </ol> <p>Depending on your needs, it is probably worth creating a dedicated Python environment for development. To do so, simply change <code>xournalpp_htr</code> from user installation manual to another name like <code>xournalpp_htr_dev</code> when you follow the above development installation steps.</p>"},{"location":"installation_user/","title":"Installation","text":"<p>This project consists of both the inference and training code. Most users will only be interested in the inference part, so that the below only comprises of the inference part that you need to execute the plugin from within Xournal++.</p> <p>The training part is optional and allows to help to train our own models which improve over time. This installation process is optional and detailed in the developer guide.</p>"},{"location":"installation_user/#linux","title":"Linux","text":"<p>Run <code>bash INSTALL_LINUX.sh</code> from repository root directory.</p> <p>This script also installs the plugin as explained in the last point of the cross-platform installation procedure. The installation of the plugin is performed with <code>plugin/copy_to_plugin_folder.sh</code>, which can also be invoked independently of <code>INSTALL_LINUX.sh</code> for updating the plugin installation.</p>"},{"location":"installation_user/#cross-platform","title":"Cross-platform","text":"<p>If you want to install the plugin manually, then execute the following commands:</p> <ol> <li>Create an environment: <code>conda create --name xournalpp_htr python=3.10.11</code>.</li> <li>Use this environment: <code>conda activate xournalpp_htr</code>.</li> <li>Install HTRPipelines package using its installation guide.</li> <li>Install all dependencies of this package <code>pip install -r requirements.txt</code>.</li> <li>Install the package in development mode with <code>pip install -e .</code> (do not forget the dot, '.').</li> <li>Install pre-commit hooks with: <code>pre-commit install</code>.</li> <li>Copy <code>plugin/</code> folder content to <code>${XOURNAL_CONFIG_PATH}/plugins/xournalpp_htr/</code> with <code>${XOURNAL_CONFIG_PATH}</code> being the configuration path of Xournal++, see Xournal++ manual here.</li> <li>Edit <code>config.lua</code>, setting <code>_M.python_executable</code> to your python executable in the conda environment and <code>_M.xournalpp_htr_path</code> to the absolute path of this repo. See the example config for details in <code>plugin/config.lua</code>.</li> <li>Ensure Xournal++ is on your <code>PATH</code>. See here for the binary location.</li> </ol>"},{"location":"installation_user/#after-installation","title":"After installation","text":"<p>Confirm that the installation worked by running <code>make tests-installation</code> from repository root directory.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>On this page, we outline the project's intended roadmap. The roadmap helps us strategically manage our time and resources.</p> <p>Below, we present our roadmap. It may evolve over time, so we will preserve previous versions to maintain transparency.</p>"},{"location":"roadmap/#roadmap-as-of-2025-11-08","title":"Roadmap as of 2025-11-08","text":"<p>Our project currently faces two main challenges: deployment and prediction quality.</p> <ol> <li> <p>Deployment: Since the plugin runs on the edge, the deployment is really the installation on the user's device. This installation has been a problem because we only tested it on Linux and the installation process requires technical knowledge. These issues have recently been mitigated through a new online demo that allows users to try Xournal++ HTR without installation. Following the successful PyInstaller proof of concept, this setup will likely become the preferred installation method in the future.</p> </li> <li> <p>Prediction Quality: We want to improve prediction quality. Improving prediction performance requires a way to measure real-life accuracy. To achieve this, two components are planned:</p> <ul> <li>A crowdsourced benchmark dataset, compiled from donated demo input data.</li> <li>An end-to-end evaluation metric to assess both text detection and transcription quality.</li> </ul> </li> </ol> <p>Additionally, we currently rely on a third-party library called <code>htr_pipeline</code> for model delivery. While effective, it complicates installation and model management. We plan to replace it with an in-house implementation. The first component, the WordDetectorNet, has already been internalized (see explanation of model, code and demo). This transition away from <code>htr_pipeline</code> will not be backward compatible.</p>"},{"location":"roadmap/#roadmap-as-of-2025-05-03","title":"Roadmap as of 2025-05-03","text":""},{"location":"roadmap/#visual-overview","title":"Visual Overview","text":"<pre><code>flowchart LR\n    A0(\n        Conduct\n        dataset\n        research\n    )\n    A(\n        Reimplement\n        &lt;a href=\"https://github.com/githubharald/HTRPipeline\"&gt;htr_pipeline&lt;/a&gt;\n    )\n    B(\n        Classic algos w\n        &lt;a href=\"https://github.com/PellelNitram/OnlineHTR\"&gt;OnlineHTR&lt;/a&gt;\n    )\n    C(\n        Start own\n        modeling\n    )\n    D(\n        Introduce\n        quality\n        measures\n    )\n    E(\n        Graph NN w\n        &lt;a href=\"https://github.com/PellelNitram/OnlineHTR\"&gt;OnlineHTR&lt;/a&gt;\n    )\n    F(\n        Make\n        installation\n        easier\n    )\n    G(\n        Explore offline\n        recognition models\n        like &lt;a href=\"https://arxiv.org/abs/1904.01941\"&gt;CRAFT&lt;/a&gt;\n    )\n    A --&gt; F\n    F --&gt; D\n    D --&gt; A0\n    A0 --&gt; C\n    C --&gt; B\n    C --&gt; E\n    C --&gt; G</code></pre>"},{"location":"roadmap/#explanation","title":"Explanation","text":"<p>This project has many potential directions, with the primary goal of delivering optimal value to users. While we are eager to implement advanced machine learning algorithms, we must first focus on usability improvements.</p> <p>Our main mid-term objective is to simplify the installation process, as users have reported it is too complex.</p> <p>Explanation of the steps:</p> <ul> <li> <p>Reimplement htr_pipeline:   We currently use the excellent htr_pipeline by Harald Scheidl for machine learning, but it being an external dependency complicates installation and them hosting model weights on Dropbox is not suitable for our needs. To address this, we plan to integrate these models directly into our project. Since the original repository lacks a license, we'll implement our own version, drawing inspiration from the existing work. This approach will deliver an easy-to-install product quickly, as we already know the requirements &amp; model details. Additionally, it enhances our understanding of training models for both online and offline handwriting data. With our own models, we'll automate model retrieval and establish a model registry, likely using Hugging Face, as part of adhering to MLOps best practices. Experimentation with new algorithms will benefit from the model registry and will occur subsequently, as it is more time-consuming.</p> </li> <li> <p>Make installation easier:   We aim to make the installation process seamless across platforms, including Linux and Windows, with future support for Mac if access becomes available to us. Implementing a model registry will streamline model management and deployment, aiding future model development and enhancing ease of use while aligning with best practices.</p> </li> <li> <p>Introduce quality measures:   To identify the best model, we need to quantify performance. Ideally, one metric will suffice, but two may be necessary if recognition and transcription remain separate tasks.</p> </li> <li> <p>Classic algos w OnlineHTR:   The plan is to use OnlineHTR for transcription alongside classical (non-data-driven) algorithms for recognition.</p> </li> <li> <p>Graph NN w OnlineHTR:   We aim to use OnlineHTR for transcription and a graph neural network for recognition. This approach seeks to develop a high-performing model that operates on the native online representation of handwriting.</p> </li> </ul>"},{"location":"user_guide/","title":"Usage","text":"<p>The usage of the project is fairly simple. First, there is a Python script that performs the actual work &amp; is useful for headless operations like batch processing. Second, and probably much more useful for the average user, the Lua plugin can be used from within Xournal++ and invokes the aforementioned Python script under the hood.</p>"},{"location":"user_guide/#the-lua-plugin","title":"The Lua plugin","text":"<p>Details relevant for usage of the Lua plugin:</p> <ol> <li>Make sure to save your file in Xournal++ beforehand. The plugin will also let you know that you need to save your file first.</li> <li>After installation, navigate to <code>Plugin &gt; Xournal++ HTR</code> to invoke the plugin. Then select a filename and press <code>Save</code>. Lastly, wait a wee bit until the process is finished; the Xournal++ UI will block while the plugin applies HTR to your file. If you opened Xournal++ through a command-line, you can see progress bars that show the HTR process in real-time.</li> </ol> <p>Note: Currently, the Xournal++ HTR plugin requires you to use a nightly build of Xournal++ because it uses upstream Lua API features that are not yet part of the stable build. Using the officially provided Nightly AppImag, see here, is very convenient. The plugin has been tested with the following nightly Linux build of Xournal++:</p> <pre><code>xournalpp 1.2.3+dev (583a4e47)\n\u2514\u2500\u2500libgtk: 3.24.20\n</code></pre>"},{"location":"user_guide/#the-python-script","title":"The Python script","text":"<p>It is located in <code>xournalpp_htr/run_htr.py</code> and it features a command line interface that documents the usage of the Python script.</p>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/","title":"Design of HuggingFace Space Dockerfile","text":"<ul> <li>Status: Ongoing</li> <li>Deciders: Martin Lellep (@PellelNitram)</li> <li>Drivers: Martin Lellep (@PellelNitram)</li> <li>PRD: None</li> <li>Date: 2025-10-04</li> </ul>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/#context","title":"Context","text":"<p>Explain the background and the context in which the decision is being made. Include any relevant information about the problem, constraints, or goals.</p>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/#decisions","title":"Decisions","text":"<p>State the decision that has been made. Be clear and concise.</p> <ul> <li>In the future, download models at build time into the Docker image from Github release page. In the   very far future, pull them from HuggingFace at run-time.</li> <li>Add <code>xournalpp</code> binary to Docker image so that the <code>xopp</code> file can be exported as PDF prior to   execution of the HTR pipeline.</li> </ul>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/#consequences","title":"Consequences","text":"<p>Describe the consequences of the decision. Include both positive and negative outcomes, as well as any trade-offs.</p>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/#alternatives-considered","title":"Alternatives Considered","text":"<p>List and briefly describe other options that were considered and why they were not chosen.</p>"},{"location":"ADRs/001_design_of_huggingface_space_dockerfile/#references","title":"References","text":"<p>Include links or references to any supporting documentation, discussions, or resources.</p>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/","title":"ADR 002 \u2013 Use Hugging Face Ecosystem for Machine Learning","text":"<ul> <li>Date: 2025-11-03</li> <li>Status: Accepted</li> <li>PRD: None</li> <li>Drivers: Martin Lellep (@PellelNitram)</li> <li>Deciders: Martin Lellep (@PellelNitram)</li> </ul>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#context","title":"Context","text":"<p>The project originally used plain PyTorch with local files for datasets and model storage. This made it hard to share and version models; once trained, they essentially lived on a hard drive with no central management or deployment integration and I had to rely on Google Drive and Dropbox links. Also, training models on different machines is cumbersome because one needs to manually download the dataset every time.</p>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#decision","title":"Decision","text":"<p>Adopt the Hugging Face ecosystem for all machine learning\u2013related components, including:</p> <ul> <li>Model Hub: hosting and versioning trained models</li> <li>Dataset Hub: storing and sharing datasets</li> <li><code>transformers</code> and <code>datasets</code> libraries: for training and data handling</li> <li>Trainer API: for standard training workflows</li> </ul> <p>Note: We need to agree on a good naming scheme for storing and retrieving models and datasets efficiently on HuggingFace. This will be the subject of a future ADR.</p>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#rationale","title":"Rationale","text":"<p>Hugging Face offers a free, community-maintained platform that is now the industry standard for open ML projects. It provides built-in versioning and sharing, making it easy to pull models directly in demos or end-user environments. The same applies to retrieving properly versioned datasets for training, benchmarking, and various demo use cases (e.g., providing sample data in Gradio applications).</p>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#consequences","title":"Consequences","text":""},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#pros","title":"Pros","text":"<ul> <li>Centralized and versioned model/dataset hosting</li> <li>Easier sharing, collaboration, and reproducibility</li> <li>Straightforward integration in deployments by letting HuggingFace download the model automatically</li> <li>Large and active community support</li> <li>One can either fully integrate the model by subclassing <code>PreTrainedModel</code> or use it as plain artifact storage of   the binary weights file</li> </ul>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#cons","title":"Cons","text":"<ul> <li>Requires learning new APIs and conventions</li> <li>Custom training routines may need workarounds</li> <li>Invest time to learn how to convert a PyTorch model into a HF model, incl pre and post processing code</li> </ul>"},{"location":"ADRs/002_use_HuggingFace_ecosystem_for_ML/#alternatives","title":"Alternatives","text":"<p>Continuing with plain PyTorch and local storage would have been simpler but lacked any versioning, reproducibility, or sharing capabilities.</p>"},{"location":"ADRs/template/","title":"ADR 002 \u2013 Use Hugging Face Ecosystem for Machine Learning","text":"<ul> <li>Date: YYYY-MM-DD</li> <li>Status: Accepted or Ongoing or Superseeded by ADR</li> <li>PRD: None</li> <li>Drivers: Name (Link to Github handle)</li> <li>Deciders: Name (Link to Github handle)</li> </ul>"},{"location":"ADRs/template/#context","title":"Context","text":"<p>(Add text here.)</p>"},{"location":"ADRs/template/#decision","title":"Decision","text":"<p>(Add text here.)</p>"},{"location":"ADRs/template/#rationale","title":"Rationale","text":"<p>(Add text here.)</p>"},{"location":"ADRs/template/#consequences","title":"Consequences","text":""},{"location":"ADRs/template/#pros","title":"Pros","text":"<p>(Add bulletpoints here).</p>"},{"location":"ADRs/template/#cons","title":"Cons","text":"<p>(Add bulletpoints here).</p>"},{"location":"ADRs/template/#alternatives","title":"Alternatives","text":"<p>(Add text here.)</p>"},{"location":"PoCs/simplify_installation_with_single_file_executable/","title":"Simplify Installation with a Single-File Executable","text":"<ul> <li>Date: 2025-11-08  </li> <li>Author: Martin Lellep (@PellelNitram)  </li> <li>Related Issue: #34</li> </ul>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#overview","title":"Overview","text":"<p>This proof of concept explores whether <code>PyInstaller</code> can be used to package the <code>xournalpp_htr</code> Python application into a single executable file.</p> <p>The goal is to simplify installation and usage by removing the need for users to manually install Python or Conda environments.</p>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#background-motivation","title":"Background &amp; Motivation","text":"<p>Currently, <code>xournalpp_htr</code> requires a working Python and Conda setup on Linux. This dependency chain can discourage non-technical users.</p> <p>A single-file binary distribution would allow users to simply download and run the program - improving accessibility and ease of use.</p>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#objective","title":"Objective","text":"<p>This PoC evaluates whether <code>PyInstaller</code> is a viable option for creating a self-contained executable of <code>xournalpp_htr</code> that runs without an external Python installation.</p>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#experiment-setup","title":"Experiment Setup","text":"<ul> <li>Environment: Ubuntu Linux (local machine)  </li> <li>Scope: Initial validation of <code>PyInstaller</code> packaging on Linux only.</li> </ul>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#commands-used","title":"Commands Used","text":"<p>From top level folder in repository:</p> <pre><code>cd xournalpp_htr\npyinstaller --onefile \\\n    --add-data \"../external/htr_pipeline/HTRPipeline/htr_pipeline/models:htr_pipeline/models\" \\\n    --hidden-import \"PIL._tkinter_finder\" \\\n    run_htr.py\n\ndist/run_htr --input-file /home/martin/data/xournalpp_htr/test_1.xoj \\\n             --output-file /home/martin/Development/xournalpp_htr/tests/test_1_from_Xpp-3.pdf\n</code></pre>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#results","title":"Results","text":"<p>\u2705 The generated single-file executable runs successfully on the local Ubuntu machine. The bundled application performs as expected, including model loading and HTR processing.</p>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#open-questions","title":"Open Questions","text":"<ul> <li>Cross-system compatibility:<ul> <li>Does the executable work on other Linux distributions or minimal environments (e.g., EC2 or GCP VMs)?</li> <li>Next step: Test the binary on a clean VM instance or inside a representative Docker image.</li> </ul> </li> <li>Integration with Xournal++:<ul> <li>The tool currently requires <code>xournalpp</code> to render <code>.xopp</code> documents to PDF prior to recognition.</li> <li>This dependency is reasonable because we are building a Xournal++ plugin here.</li> <li>Possible approaches:<ul> <li>Require <code>xournalpp</code> to be installed and accessible via <code>$PATH</code>.</li> <li>Allow the user to specify a custom <code>xournalpp</code> binary (e.g., an AppImage).</li> </ul> </li> </ul> </li> <li>Bundling external dependencies:<ul> <li>Could the <code>xournalpp</code> binary itself be packaged within the <code>PyInstaller</code> bundle?</li> <li>Requires exploration of size implications and licensing considerations.</li> </ul> </li> </ul>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#next-steps","title":"Next Steps","text":"<ol> <li>Integration testing:<ul> <li>Validate the binary on clean Linux VMs.</li> <li>Confirm compatibility with <code>xournalpp</code> when executed via different paths.</li> </ul> </li> <li>Cross-platform builds:<ul> <li>Attempt <code>PyInstaller</code> builds for macOS and Windows.</li> <li>Automate using GitHub Actions to produce platform-specific binaries.</li> </ul> </li> <li>Plugin integration:<ul> <li>Update the Lua part of the Xournal++ HTR plugin to use the new standalone executable.</li> </ul> </li> <li>Documentation update:<ul> <li>Document the build and installation process once validated.</li> </ul> </li> </ol>"},{"location":"PoCs/simplify_installation_with_single_file_executable/#summary","title":"Summary","text":"<p>This PoC confirms that <code>PyInstaller</code> is a viable solution for packaging <code>xournalpp_htr</code> into a single-file binary on Linux. The next phase will focus on integration testing, cross-platform builds, and automating releases to make distribution fully user-friendly.</p>"}]}